{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b19bd1-36d7-4a82-868f-37a19229ebc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running an API endpoint locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e887e-7c40-486f-aa61-8b146052bc73",
   "metadata": {},
   "source": [
    "This notebook is a simple walkthrough of how to serve a local API endpoint with FastAPI, using a publicly available Graphcore model on the IPU, from a Docker image. \n",
    "\n",
    "Here, we'll cover:\n",
    "\n",
    "* Pulling the example image for [Stable Diffusion 2 Text-to-image]() for inference \n",
    "* Running up the FastAPI service on your machine from the image to create a locally hosted endpoint\n",
    "* How to access and send requests to the endpoint and receive model output\n",
    "\n",
    "The public model inference images available on Graphcore's Docker Hub have all of the necessary dependencies 'baked in', including executables and model binaries, to make the process of serving up an endpoint as smooth as possible. The internals of the image are based on the [api-deployment]() repository model-serving architecture. This is designed to be a straightforward example of serving a model with FastAPI and running up a local endpoint. Once you've tested your local endpoint functionality, you can use the same container to launch up a deployment in [Paperspace]()!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcb904-d435-4fd8-9275-37689d40f9c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before starting the notebook, install the necessary dependencies to run the endpoint demo. We will use [Gradio]() to create a basic in-notebook GUI using the Stable Diffusion model endpoint served by the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7a454-e763-4264-bc0d-8e252dadff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gradio\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8a776-1ae0-473f-bf65-3076c58bcb04",
   "metadata": {
    "tags": []
   },
   "source": [
    "Specify the address of the docker image to pull using an environment variable, this is simply the Docker Hub username ('Graphcore') followed by the name of the container. We can easily run bash commands using the `!` denotation in notebooks to get the image into the notebook environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c8a88-fbaa-492d-8c98-c201aaa400c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env DOCKERHUB_IMAGE=gcapidev/stable-diffusion-2-512-deployment\n",
    "%env LOCAL_IP_ADDRESS=localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a3e7e-4fa5-4961-842a-255603e77dc6",
   "metadata": {},
   "source": [
    "## Pull the image from Docker Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d2ff8-a0da-4d6b-83a4-f452b48a5f77",
   "metadata": {},
   "source": [
    "Ensure the environment variable is set correctly, we can print it with `echo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167dfeb-93e1-4d2c-96a7-08d83e5ee6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! echo $DOCKERHUB_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803f213-de2e-47f2-8acf-b29a773fac32",
   "metadata": {},
   "source": [
    "Next, use the `docker pull` command to get the image from the Graphcore image repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727a4f5-497b-4c97-a9ab-f46494e47577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker pull $DOCKERHUB_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28778b-964c-4d72-8d5f-1b92dfdb40f7",
   "metadata": {},
   "source": [
    "To check the image has been pulled into the environment, we can check the list of locally available docker images and use `grep` to search for the name of our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0d4fc-2997-4494-8419-171160fed9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker image list | grep $DOCKERHUB_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfbda4-4192-443b-be16-6aee186c596f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e89d75-59e5-4809-a840-f0d0fc58d450",
   "metadata": {},
   "source": [
    "The image we pulled has the model, executables and FastAPI endpoint built in. When we run the image, it will start preparing the model, loading the executables and run up the endpoint. To send requests to the endpoint, we first need to wait until the model is ready to receive requests, once all the binaries have been loaded and model graphs compiled, the console output will show the IP address that the endpoint is being served at.\n",
    "\n",
    "In this case, there is only one model present in the image, but some images may contain multiple models, serving up multiple endpoints for use. You can modify which models the image should prepare by specifying the `SERVER_MODELS` environment variable within the `docker run` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cc450-60e0-4ae1-ba9b-dcf79c6893fe",
   "metadata": {},
   "source": [
    "Ensure the environment variables `IPUOF_VIPU_API_HOST` and `IPUOF_VIPU_API_PARTITION_ID` are set on the host machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75725987-fa40-46d0-b662-1b6c1ab95f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! echo $IPUOF_VIPU_API_HOST\n",
    "! echo $IPUOF_VIPU_PARTITION_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0de3b-5a7e-4540-83c0-f96e50c2d284",
   "metadata": {},
   "source": [
    "`-e` indicates environment variables that need to be set for the image, including the `POPTORCH_CACHE_DIR`, `HUGGINGFACE_HUB_CACHE` and `HF_HOME` - these are the model cache directory environment variables recognised by the Poptorch framework and the Hugging Face model used in the container. \n",
    "\n",
    "The last line of the command specifies the name of the image we want to run up, in this case, our downloaded image from the Docker Hub repository. In this notebook, we will run the image in a **detached** state using `-d`, this is because the model endpoint images do not 'finish' running, and as such the output to `stdout` doesn't end, meaning the following cell would never stop running if the command is run attached to the terminal. The terminal output is, however, useful to view, and will let you know when the image is ready to receive requests, when `uvicorn` is ready. \n",
    "\n",
    "You can run this step on the notebook terminal by clicking the small notebook symbol on the left navigation bar in your Paperspace console, running the command in the attached state to view the live output:\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "    -e POPTORCH_CACHE_DIR=/src/model_cache \\\n",
    "    -e HUGGINGFACE_HUB_CACHE=/src/model_cache/ \\\n",
    "    -e HF_HOME=/src/model_cache/ \\\n",
    "    --env-file <(env | grep IPU) \\\n",
    "    --network host \\\n",
    "    --device=/dev/infiniband/ \\\n",
    "    --cap-add=IPC_LOCK \\\n",
    "    $DOCKERHUB_IMAGE\n",
    "```\n",
    "\n",
    "If you run the image in the terminal, skip the next cell to ensure you're not running up two images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cfb3e-fa0f-4b69-965a-d979131d34dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the image in the notebook in a detached state, you will not be able to view terminal output:\n",
    "\n",
    "! docker run -d \\\n",
    "    -e POPTORCH_CACHE_DIR=/src/model_cache \\\n",
    "    -e HUGGINGFACE_HUB_CACHE=/src/model_cache/ \\\n",
    "    -e HF_HOME=/src/model_cache/ \\\n",
    "    --env-file <(env | grep IPU) \\\n",
    "    --network host \\\n",
    "    --device=/dev/infiniband/ \\\n",
    "    --cap-add=IPC_LOCK \\\n",
    "    $DOCKERHUB_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efd4fc-49a5-40f2-b5ea-0eb362c663ed",
   "metadata": {},
   "source": [
    "You can now view the running containers and container IDs, including the above one, by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f0a71-2356-4dbf-863d-884f6ba7c3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72cd8c5-1b70-42cd-b0a4-95110a2d0030",
   "metadata": {},
   "source": [
    "Remember that when you are finished with the container, remember to stop and then delete the container, ensuring it detaches from any devices it is attached to.\n",
    "\n",
    "From the listed containers output by the `docker ps` command, find your container by the `IMAGE` column, uncomment and replace `<container ID>` in the following lines with the corresponding ID from `CONTAINER ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80424965-7dd1-499f-8d7c-17fd214964cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# docker stop <container ID>\n",
    "# docker rm <container ID>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d3a5e-9660-4dc2-b181-be0b33fda75b",
   "metadata": {},
   "source": [
    "## Using the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db983e4e-4164-4356-9ff5-53505c5fd695",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, import the necessary packages for this stage. We will use the easy `requests` package to send requests to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5e733-c6cd-4483-9444-776675dfd300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27269d27-d2eb-4177-a81f-c2b04d050047",
   "metadata": {},
   "source": [
    "Once the model has been initialised and compiled, the endpoint is successfully running. We can ensure the model is running by performing a `GET` request to the `/readiness` service, this is a health check which will return the endpoints state when called.\n",
    "\n",
    "The server is not immediately ready to use, so after running `docker run` in a detached state, we need to wait for the endpoint to be ready before proceeding to actually perform inference with the endpoint. For this, we use a simple function to loop until the `GET` request to the health check returns the passing message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9413d-688a-4e6d-974e-829421618f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_readiness(url):\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(f\"{url}/readiness\")\n",
    "            response = response.json()\n",
    "            if response['message'] == 'Readiness check succeeded.': \n",
    "                print(f\"Server ready - {response['message']}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Server waiting - {response['message']}\")\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "        \n",
    "    return True\n",
    "\n",
    "print(\"Waiting for readiness...\")\n",
    "\n",
    "warmup_start = time.perf_counter()\n",
    "ready = wait_for_readiness(\"http://localhost:8100\")\n",
    "\n",
    "print(f\"Warm up time: {time.perf_counter() - warmup_start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f4748-6e4f-4fc6-86a7-653f982b2475",
   "metadata": {},
   "source": [
    "The message should say 'Readiness check succeeded', which means we are ready to start generating images with the model using the live endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92094eac-e21f-482d-aa3f-a80aaa4811e7",
   "metadata": {},
   "source": [
    "Lets create a dictionary for the parameters to send to the model. This is specific to and defined by the model endpoint that has been created. For Stable Diffusion, we must pass:\n",
    "* `prompt`: Main body of text describing the image we want to create.\n",
    "* `random_seed`: Can be used to emulate a deterministic image output from the same prompt each time (we set this to random to observe variation in the image).\n",
    "* `guidance scale`: Specific to Stable Diffusion, it controls how strongly the generated image will follow the text output. \n",
    "* `return_json`: Defines whether to return a JSON object in the response or not, to receive an encoded image, we want to set this to `True`. \n",
    "* `negative_prompt`: Defines any aspects we don't want to see in the image.\n",
    "* `num_inference_steps`: The number of sampling steps undertaken by the model, increasing this up to a point should improve the image quality of the generated image, 25-50 steps is a reasonable range for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2036f-8cd1-41cd-b495-a9753c7eb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "      \"prompt\": \"big red dog\",\n",
    "      \"random_seed\": random.randint(0,99999999),\n",
    "      \"guidance_scale\": 9,\n",
    "      \"return_json\": True,\n",
    "      \"negative_prompt\": \"string\",\n",
    "      \"num_inference_steps\": 25\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889652db-2911-4b8e-a48c-38f65619ce1e",
   "metadata": {},
   "source": [
    "Next, we can use `requests` to send a POST call to the REST endpoint at the IP address that the endpoint is running on. This will return an image in the response JSON body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dad08-2cd5-4a14-8ee7-6467acba978e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\"http://10.129.96.114:8100/stable_diffusion_2_txt2img_512\", json=model_params)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(response.status_code)\n",
    "    \n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c3aa3-562e-42fd-9ae1-f7dd3d07c5d3",
   "metadata": {},
   "source": [
    "Now, the image has been returned in Base64 encoded form within the JSON, we can decode this using the `base64` and `io` libraries to visualise the image. First, we decode the images returned by the model and convert them to PIL RGB images - in this case there is only one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d339eb-389d-4196-b61d-49f705433f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "images_b64 = [i for i in response['images']]\n",
    "\n",
    "pil_images = []\n",
    "for b64_img in images_b64:\n",
    "    base64bytes = base64.b64decode(b64_img)\n",
    "    bytesObj = io.BytesIO(base64bytes)\n",
    "    img = Image.open(bytesObj)\n",
    "    \n",
    "    pil_images.append(img)\n",
    "    \n",
    "print(\"Number of images returned: \", len(pil_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd16df5-3f1b-4f6a-9084-f5b4c221d476",
   "metadata": {},
   "source": [
    "Once the images have been converted, they can be viewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0601a67-18f2-4331-8a5d-ba6d6bf46845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(pil_images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2a5e5-e464-4c78-98c6-e7802855e517",
   "metadata": {},
   "source": [
    "Once we have tested the endpoint as above, we have all the tools needed to create a basic interface capability to infer from the model through the endpoint, with a GUI. Lets create a simple Gradio app for this, wrapping the POST request to the model and the image decoder into a short and simple function which will serve as the core of the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588274d-f353-4db4-9fdd-d18962eb6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "def stable_diffusion_2_inference(prompt, guidance_scale, num_inference_steps):\n",
    "    model_params = {\n",
    "      \"prompt\": prompt,\n",
    "      \"random_seed\": random.randint(0,99999999),\n",
    "      \"guidance_scale\": guidance_scale,\n",
    "      \"return_json\": True,\n",
    "      \"negative_prompt\": \"string\",\n",
    "      \"num_inference_steps\": num_inference_steps\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\"http://localhost:8100/stable_diffusion_2_txt2img_512\", json=model_params)\n",
    "    response = response.json()\n",
    "    \n",
    "    images_b64 = [i for i in response['images']]\n",
    "    pil_images = []\n",
    "    for b64_img in images_b64:\n",
    "        base64bytes = base64.b64decode(b64_img)\n",
    "        bytesObj = io.BytesIO(base64bytes)\n",
    "        img = Image.open(bytesObj)\n",
    "\n",
    "        pil_images.append(img)\n",
    "    \n",
    "    return np.array(pil_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ca410-9dc0-4bea-828c-2bc76c9ff7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n",
    "demo = gr.Interface(\n",
    "    fn=stable_diffusion_2_inference, \n",
    "    inputs=[gr.Textbox(value=\"Ice skating on the moon\"),\n",
    "            gr.Slider(1,50,value=9, step=1, label='Guidance scale'),\n",
    "            gr.Slider(1,100,value=25, step=1, label='Number of steps')\n",
    "           ], \n",
    "    outputs=gr.Image(shape=(512,512))\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64320c2-edc4-4ac5-9724-8819f4585a2c",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "To recap, this notebook has covered how to pull a public Docker image with a Graphcore model endpoint, serve the endpoint on your local machine, observe readiness of the endpoint, and finally send prompts to the API and predict using the model from the endpoint, receiving the output into your workspace or as a simple frontend for the model using GUIs like Gradio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
